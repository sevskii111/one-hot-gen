{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InterfaxHack_Use.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io-WTWsXm0nr",
        "outputId": "b90214a5-762e-4b8e-e30c-f1bb02c5230d"
      },
      "source": [
        "!pip install -Uq transformers rich[jupyter] sentencepiece gdown flask-ngrok"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 67.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 75.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.8 MB/s \n",
            "\u001b[?25h  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBhD4qKwm2LQ"
      },
      "source": [
        "MAX_SOURCE_TEXT_LENGTH = 512\n",
        "MAX_TARGET_TEXT_LENGTH = 17\n",
        "NEWS_PER_STORY_PUBLIC = 5\n",
        "NEWS_PER_STORY_OTHER = 1\n",
        "BATCH_SIZE = 8\n",
        "TRAIN_EPOCHS = 3\n",
        "NUM_BEAMS = 2\n",
        "\n",
        "OUTPUT_DIR = 'output_dir'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzAJZYHvnCKY"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWLs6S7dnYmC",
        "outputId": "2430df6b-9449-4a86-b299-1d7220566b4f"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1ekYuFbcJnpwMT5CcIxgaOc6hprBen9ij\n",
        "!gdown https://drive.google.com/uc?id=1a4WdZyQ5zdJ_S7oGdJUMLR-zqW8FwkKx\n",
        "!tar xvf model.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ekYuFbcJnpwMT5CcIxgaOc6hprBen9ij\n",
            "To: /content/model.tar.gz\n",
            "905MB [00:04, 202MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1a4WdZyQ5zdJ_S7oGdJUMLR-zqW8FwkKx\n",
            "To: /content/tfidf.pickle\n",
            "5.35MB [00:00, 126MB/s]\n",
            "output_dir/model_files/\n",
            "output_dir/model_files/config.json\n",
            "output_dir/model_files/spiece.model\n",
            "output_dir/model_files/pytorch_model.bin\n",
            "output_dir/model_files/tokenizer_config.json\n",
            "output_dir/model_files/special_tokens_map.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLEzKwMDnXkO"
      },
      "source": [
        "PATH = Path().absolute() / 'output_dir' / 'model_files'\n",
        "model = T5ForConditionalGeneration.from_pretrained(PATH)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(PATH)\n",
        "\n",
        "def generate(texts, **kwargs):\n",
        "    #inputs = tokenizer(text, return_tensors='pt')\n",
        "    results = []\n",
        "    for i in range(0, len(texts), BATCH_SIZE):\n",
        "      texts_batch = texts[i:i + BATCH_SIZE]\n",
        "\n",
        "      source = tokenizer.batch_encode_plus(\n",
        "            texts_batch,\n",
        "            max_length=MAX_SOURCE_TEXT_LENGTH,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "      ids = source[\"input_ids\"].squeeze().to(device, dtype = torch.long)\n",
        "      mask = source[\"attention_mask\"].squeeze().to(device, dtype = torch.long)\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask, \n",
        "          max_length=MAX_TARGET_TEXT_LENGTH, \n",
        "          num_beams=NUM_BEAMS,\n",
        "          repetition_penalty=1.0, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      results += preds\n",
        "      \n",
        "    return results"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZU1f2HIw9nv"
      },
      "source": [
        "def predict_easy(df):\n",
        "  df = df.copy()\n",
        "  max_words = 5\n",
        "  for w in range(1, max_words + 1):\n",
        "      df[f\"{w}_words\"] = df['X'].apply(lambda h: ' '.join(h.split(' ')[:w]))\n",
        "\n",
        "  titles = df['y'].unique()\n",
        "\n",
        "  result = {\n",
        "      'title': list(),\n",
        "      'result': list(),\n",
        "  }\n",
        "\n",
        "  c = 1.6\n",
        "  coeffs = [0.1 * c, 0.15 * c, 0.2 * c, 0.15 * c, 0.1 * c]\n",
        "\n",
        "  for title in titles:\n",
        "      samples = df[df['y'] == title]\n",
        "      sum_samples = len(samples)\n",
        "      for w in reversed(range(1, max_words + 1)):\n",
        "          u = len(samples[f'{w}_words'].unique())\n",
        "          coeff = u / sum_samples / np.log(sum_samples)\n",
        "          res = list(samples[f'{w}_words'].value_counts().items())[0][0]\n",
        "          if coeff <= coeffs[w - 1] and res.split(' ')[-1].lower() not in ['на', 'в']:\n",
        "              result['title'].append(title)\n",
        "              if res[-1] == ':':\n",
        "                  res = res[:-1]\n",
        "              \n",
        "              result['result'].append(res)\n",
        "              break\n",
        "  return pd.DataFrame(result)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCmiH-IkyXzt"
      },
      "source": [
        "def predict_t5(df):\n",
        "  preds = generate(df[\"X\"].values)\n",
        "  with open('./tfidf.pickle', 'rb') as handle:\n",
        "    tfidf = pickle.load(handle)\n",
        "  predictions = pd.DataFrame()\n",
        "  predictions[\"Actual Text\"] = df['y']\n",
        "  predictions[\"Generated Text\"] = preds\n",
        "  gts = predictions[\"Actual Text\"].unique()\n",
        "\n",
        "  feature_names = np.array(tfidf.get_feature_names())\n",
        "\n",
        "  def get_top_tf_idf_words(response, top_n=2):\n",
        "      sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
        "      return feature_names[response.indices[sorted_nzs]]\n",
        "\n",
        "  dl_results = []\n",
        "\n",
        "  for gt in gts:\n",
        "    curr_preds = predictions[predictions['Actual Text'] == gt][\"Generated Text\"]\n",
        "    t_text = tfidf.transform(['. '.join(curr_preds.values)])\n",
        "\n",
        "    top_words = get_top_tf_idf_words(t_text, 2)\n",
        "    variants = []\n",
        "    for pred in curr_preds:\n",
        "      pred_words = pred.lower().split(' ')\n",
        "      i = len(set(top_words).intersection(set(pred_words)))\n",
        "      if len(pred_words) > 1:\n",
        "        i /= len(pred_words)\n",
        "      variants.append((i, pred, gt))\n",
        "    res = sorted(variants, reverse=True)[0]\n",
        "    dl_results.append((res[1:]))\n",
        "\n",
        "  return pd.DataFrame(dl_results, columns=[\"result\", \"title\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oervk7kh3FX7"
      },
      "source": [
        "def predict(df):\n",
        "  easy_preds = predict_easy(df)\n",
        "  t5_preds = predict_t5(df)\n",
        "\n",
        "  result = list()\n",
        "\n",
        "  for story_id in df[\"y\"].unique():\n",
        "    easy_pred = easy_preds[easy_preds[\"title\"] == story_id]\n",
        "    if len(easy_pred) > 0:\n",
        "      result.append((story_id, easy_pred.iloc[0][\"result\"]))\n",
        "    else:\n",
        "      result.append((story_id, t5_preds[t5_preds[\"title\"] == story_id].iloc[0][\"result\"]))\n",
        "\n",
        "  return pd.DataFrame(result, columns=[\"story_id\", \"story_name\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6WbagfiOFAA",
        "outputId": "573adbe5-767e-4d65-e479-1db72c09821e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sevskii111/one-hot-gen/main/frontend/index.html -O index.html\n",
        "!wget https://raw.githubusercontent.com/sevskii111/one-hot-gen/main/frontend/main.js -O main.js"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-22 06:39:01--  https://raw.githubusercontent.com/sevskii111/one-hot-gen/main/frontend/index.html\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9507 (9.3K) [text/plain]\n",
            "Saving to: ‘index.html’\n",
            "\n",
            "\rindex.html            0%[                    ]       0  --.-KB/s               \rindex.html          100%[===================>]   9.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-22 06:39:01 (93.9 MB/s) - ‘index.html’ saved [9507/9507]\n",
            "\n",
            "--2021-08-22 06:39:01--  https://raw.githubusercontent.com/sevskii111/one-hot-gen/main/frontend/main.js\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1038661 (1014K) [text/plain]\n",
            "Saving to: ‘main.js’\n",
            "\n",
            "main.js             100%[===================>]   1014K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-08-22 06:39:01 (20.8 MB/s) - ‘main.js’ saved [1038661/1038661]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y72ciqZv64rd",
        "outputId": "eee209de-2a56-4dfa-cc8c-644aebfe04a9"
      },
      "source": [
        "import sys\n",
        "from flask import Flask, send_file, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   \n",
        "  \n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return send_file('index.html')\n",
        "\n",
        "@app.route(\"/main.js\")\n",
        "def main():\n",
        "    return send_file('main.js')\n",
        "\n",
        "@app.route(\"/get_preds\", methods=[\"POST\"])\n",
        "def get_preds():\n",
        "  df = pd.DataFrame(request.json)\n",
        "  preds = predict(df)\n",
        "  return preds.to_json()\n",
        "    \n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://43b4-34-74-84-28.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [22/Aug/2021 06:40:11] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Aug/2021 06:40:11] \"\u001b[37mGET /main.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Aug/2021 06:40:12] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "127.0.0.1 - - [22/Aug/2021 06:40:32] \"\u001b[37mPOST /get_preds HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [22/Aug/2021 06:47:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "127.0.0.1 - - [22/Aug/2021 06:47:55] \"\u001b[37mPOST /get_preds HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIvsDaVNAptn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}